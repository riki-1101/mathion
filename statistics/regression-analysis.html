<!DOCTYPE html>
<html lang="ja">
    <head>
        <title>単回帰分析</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="../css/style.css">
        <script src="../js/mathjax-config.js"></script>
        <script src="../mathjax/es5/tex-mml-chtml.js"></script>
    </head>
    <body>
        <header id="header"></header>
        <main id="contents">
            <h1>単回帰分析</h1>
            <div id="table-of-contents"></div>
            <h2>回帰分析とは</h2>
            <div class="article">
                回帰分析とは、ある特定の変数を他の変数から予測する手法である。予測したい変数を目的変数といい、予測に用いる変数を説明変数という。
                予測式として線形式を用いる場合を線形回帰といい、非線形な予測式を用いる場合を非線形回帰という。
            </div>
            <div class="article">
                目的変数 \(y\) の予測に説明変数 \(x\) を用いる。
                予測式は
                \[
                y=ax+b
                \]
                \(x_i\) に対する予測値は
                \[
                \hat{y_i}=ax_i+b
                \]
                と求められる。
                観測値から予測値を引いたものを残差という。
                \[
                \varepsilon_i=y_i-\hat{y_i}=y_i-(ax_i+b)
                \]
                各データに対する残差の平方和を最小にすることを考える。
                \[
                \mathrm{RSS}=\sum_{i=1}^n\varepsilon_i^2=\sum_{i=1}^n\{y_i-(ax_i+b)\}^2
                \]
                これが最小になるように、係数 \(a,b\) の値を求める推定法を最小二乗法という。
                <p></p>
                ここで、次の関数を考える。
                \[
                E(a,b)=\frac{1}{2}\sum_{i=1}^n(ax_i+b-y_i)^2
                \]
                偏微分すると
                \[
                \begin{align}
                \begin{cases}
                \displaystyle\frac{\partial E}{\partial a}=\sum_{i=1}^nx_i(ax_i+b-y_i)=0\\
                \displaystyle\frac{\partial E}{\partial a}=\sum_{i=1}^n(ax_i+b-y_i)=0
                \end{cases}
                &\Leftrightarrow\begin{cases}
                a\displaystyle\sum_{i=1}^nx_i^2+b\sum_{i=1}^nx_i=\sum_{i=1}^nx_iy_i\\
                a\displaystyle\sum_{i=1}^nx_i+bn=\sum_{i=1}^ny_i
                \end{cases}\\
                \end{align}
                \]
                第２式より
                \[
                b=\frac{1}{n}\sum_{i=1}^ny_i-a\cdot\frac{1}{n}\sum_{i=1}^nx_i=\overline{y}-a\overline{x}
                \]
                第１式の両辺を \(n\) で割ると
                \[
                a\cdot\frac{1}{n}\sum_{i=1}^nx_i^2+b\cdot\frac{1}{n}\sum_{i=1}^nx_i=\frac{1}{n}\sum_{i=1}^nx_iy_i
                \]
                \[
                a\overline{x^2}+b\overline{x}=\overline{xy}
                \]
                これに \(b=\overline{y}-a\overline{x}\) を代入して
                \[
                a\overline{x^2}+(\overline{y}-a\overline{x})\overline{x}=\overline{xy}
                \]
                \[
                a\{\overline{x^2}-(\overline{x})^2\}=\overline{xy}-\overline{x}\cdot\overline{y}
                \]
                よって
                \[
                a=\frac{\overline{xy}-\overline{x}\cdot\overline{y}}{\overline{x^2}-(\overline{x})^2},~~~~~b=\overline{y}- \frac{\overline{xy}-\overline{x}\cdot\overline{y}}{\overline{x^2}-(\overline{x})^2} \overline{x}
                \]
            </div>
            <p></p>
            <h2>重回帰分析</h2>
            <div class="article">
                複数の説明変数を使って１つの目的変数を予測する回帰分析の手法を重回帰分析という。
                <p></p>
                説明変数 \(x_1,x_2,\cdots,x_p\)
                <p></p>
                <table border="1">
                    <tr><th>          </th><th>\(y\)     </th><th>\(x_1\)   </th><th>\(x_2\)   </th><th>\(\cdots\)</th><th>\(x_p\)   </th></tr>
                    <tr><td>1         </td><td>\(y_1\)   </td><td>\(x_{11}\)</td><td>\(x_{12}\)</td><td>\(\cdots\)</td><td>\(x_{1p}\)</td></tr>
                    <tr><td>2         </td><td>\(y_2\)   </td><td>\(x_{21}\)</td><td>\(x_{22}\)</td><td>\(\cdots\)</td><td>\(x_{2p}\)</td></tr>
                    <tr><td>\(\vdots\)</td><td>\(\vdots\)</td><td>\(\vdots\)</td><td>\(\vdots\)</td><td>\(\ddots\)</td><td>\(\vdots\)</td></tr>
                    <tr><td>n         </td><td>\(y_n\)   </td><td>\(x_{n1}\)</td><td>\(x_{n2}\)</td><td>\(\cdots\)</td><td>\(x_{np}\)</td></tr>
                </table>
                <p></p>
                回帰式は
                \[
                y=\beta_0+\beta_1x_1+\beta_2x_2+\cdots+\beta_px_p
                \]
                と表され、\(\beta_0,\beta_1,\cdots,\beta_p\) は偏回帰係数と呼ばれる。
                <p></p>
                \[
                X=
                \begin{bmatrix}
                1 & x_{11} & x_{12} & \cdots & x_{1p}\\
                1 & x_{21} & x_{22} & \cdots & x_{2p}\\
                \vdots & \vdots & \vdots & \ddots & \vdots\\
                1 & x_{n1} & x_{n2} & \cdots & x_{np}\\
                \end{bmatrix}
                \]
            </div>
        </main>
        <footer id="footer"></footer>
        <script src="../js/header.js"></script>
        <script src="../js/footer.js"></script>
        <script src="../js/script.js"></script>
    </body>
</html>